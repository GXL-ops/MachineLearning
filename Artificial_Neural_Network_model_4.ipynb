{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 循环神经网络(RNN)\n",
    "前面已经说明全连接神经网络和卷积神经网络，在训练和预测阶段，都只是单独的取出每个输入，经过处理后\n",
    "给出输出，前一个输入和后一个输入没有任何关系，在输入上可以随机。\n",
    "\n",
    "但是对于一些任务需要前一个输入和后一个输入有关联关系。例如理解一句话时，单个单词是不够的，以及处理视频也是如此\n",
    "\n",
    "循环神经网络增加了循环机制，使得信号从一个神经元传递到另一个神经元后，其值不会马上消失，而是继续存活，\n",
    "以达到输入前后相关联的目的。循环神经网络会将隐层的内部神经元连接起来，使得隐层的输入不仅仅包含上一层的输出\n",
    "，还包括上一时刻隐层的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\nW1115 10:06:12.539461 11132 deprecation.py:323] From <ipython-input-1-f6d522b1203c>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W1115 10:06:12.542674 11132 deprecation.py:323] From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease write your own downloading logic.\n",
      "W1115 10:06:12.545664 11132 deprecation.py:323] From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\n",
      "W1115 10:06:13.139610 11132 deprecation.py:323] From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\n",
      "W1115 10:06:13.148594 11132 deprecation.py:323] From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.one_hot on tensors.\n",
      "W1115 10:06:13.237348 11132 deprecation.py:323] From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\nExtracting MNIST_data\\t10k-images-idx3-ubyte.gz\nExtracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "number of train data is 55000\nnumber of test data is 10000\nMNIST ready\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "print(\"number of train data is %d\"%(mnist.train.num_examples))\n",
    "print(\"number of test data is %d\"%(mnist.test.num_examples))\n",
    "trainimg=mnist.train.images\n",
    "trainlabel=mnist.train.labels\n",
    "testimg=mnist.test.images\n",
    "testlabel=mnist.test.labels\n",
    "print(\"MNIST ready\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 构建神经网络模型\n",
    "LSTM网络模型由一个输入层、一个全连接神经网络层、一个LSTM层和一个输出层组成。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-739a06d5911b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0mpred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;31m# 定义损失函数和优化方法，其中损失函数使用sotfmax交叉熵，优化方法为Adam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-739a06d5911b>\u001b[0m in \u001b[0;36mRNN\u001b[1;34m(_X, _weights, _biases)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# state： 最终的状态。一般情况下state的形状为 [batch_size, cell.output_size ]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m# 如果cell是LSTMCells,则state将是包含每个单元格的LSTMStateTuple的元组，state的形状为[2，batch_size, cell.output_size ]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_cell\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_in\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minit_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtime_major\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;31m# 输出层\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mresults\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'out'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0m_biases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'out'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[1;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[0;32m    662\u001b[0m         raise ValueError(\n\u001b[0;32m    663\u001b[0m             \u001b[1;34m\"sequence_length must be a vector of length batch_size, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m             \"but saw shape: %s\" % sequence_length.get_shape())\n\u001b[0m\u001b[0;32m    665\u001b[0m       sequence_length = array_ops.identity(  # Just to find it in the graph.\n\u001b[0;32m    666\u001b[0m           \u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: sequence_length must be a vector of length batch_size, but saw shape: (2, 100, 128)"
     ],
     "ename": "ValueError",
     "evalue": "sequence_length must be a vector of length batch_size, but saw shape: (2, 100, 128)",
     "output_type": "error"
    }
   ],
   "source": [
    "# RNN神经网络参数\n",
    "# 输入层的数量\n",
    "n_input=28\n",
    "n_steps=28\n",
    "# 隐层的数量\n",
    "n_hidden=128\n",
    "# 输出的数量，因为是分类问题，这里一共有10类\n",
    "n_classes=10\n",
    "batch_size=100\n",
    "\n",
    "x=tf.placeholder(tf.float32,[None,n_steps,n_input])\n",
    "y=tf.placeholder(tf.float32,[None,n_classes])\n",
    "# 随机初始化每层权重值和偏置量\n",
    "weights={\n",
    "    'hidden':tf.Variable(tf.random_normal([n_input,n_hidden])),\n",
    "    'out':tf.Variable(tf.random_normal([n_hidden,n_classes]))\n",
    "}\n",
    "biases={\n",
    "    'hidden':tf.Variable(tf.constant(0.1,shape=([n_hidden,]))),\n",
    "    'out':tf.Variable(tf.constant(0.1,shape=([n_classes,])))\n",
    "}\n",
    "def RNN(_X,_weights,_biases):\n",
    "    _X=tf.reshape(_X,[-1,n_input])\n",
    "    # 输入层到隐层，第一次直接运算\n",
    "    X_in=tf.matmul(_X,_weights['hidden'])+_biases['hidden']\n",
    "    # 规则数据\n",
    "    X_in=tf.reshape(X_in,[-1,n_steps,n_hidden])\n",
    "    # 之后使用LSTM，定义一个 LSTM 结构，LSTM 中使用的变量会在该函数中自动被声明\n",
    "    lstm_cell=tf.contrib.rnn.BasicLSTMCell(n_hidden,forget_bias=1.0,state_is_tuple=True)\n",
    "    # 初始化，将 LSTM 中的状态初始化为全 0 数组，batch_size 给出一个 batch 的大小\n",
    "    init_state=lstm_cell.zero_state(batch_size,dtype=tf.float32)\n",
    "    # 使用dynamic_rnn方法，执行RNN运算\n",
    "    # 一对(outputs, state),其中：\n",
    "    # outputs： RNN输出Tensor。如果time_major == False(默认),这将是shape为[batch_size, max_time, cell.output_size]的Tensor.\n",
    "    # 如果time_major == True,这将是shape为[max_time, batch_size, cell.output_size]的Tensor.\n",
    "    # state： 最终的状态。一般情况下state的形状为 [batch_size, cell.output_size ]\n",
    "    # 如果cell是LSTMCells,则state将是包含每个单元格的LSTMStateTuple的元组，state的形状为[2，batch_size, cell.output_size ]\n",
    "    outputs,final_state=tf.nn.dynamic_rnn(lstm_cell,X_in,init_state=init_state,time_major=False)\n",
    "    # 输出层\n",
    "    results=tf.matmul(final_state[1],_weights['out'])+_biases['out']\n",
    "    return results\n",
    "\n",
    "pred=RNN(x,weights,biases)\n",
    "# 定义损失函数和优化方法，其中损失函数使用sotfmax交叉熵，优化方法为Adam\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "进行数据训练和评估模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "step 0,training accuracy 0.13\n",
      "test accuracy 0.1009\n",
      "step 100,training accuracy 0.89\n",
      "test accuracy 0.8939\n",
      "step 200,training accuracy 0.92\n",
      "test accuracy 0.9215\n",
      "step 300,training accuracy 0.94\n",
      "test accuracy 0.9363\n",
      "step 400,training accuracy 0.95\n",
      "test accuracy 0.9435\n",
      "step 500,training accuracy 0.94\n",
      "test accuracy 0.9507\n",
      "step 600,training accuracy 0.99\n",
      "test accuracy 0.9516\n",
      "step 700,training accuracy 0.98\n",
      "test accuracy 0.9484\n",
      "step 800,training accuracy 0.92\n",
      "test accuracy 0.9461\n",
      "step 900,training accuracy 0.97\n",
      "test accuracy 0.9529\n",
      "step 1000,training accuracy 0.93\n",
      "test accuracy 0.9518\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# 进行模型评估\n",
    "correct_pred=tf.equal(tf.argmax(pred,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "# 初始化\n",
    "init=tf.global_variables_initializer()\n",
    "# 开始运行\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # 每次选择100个样本\n",
    "    display_step=100\n",
    "    step=0\n",
    "    train_iters=int(mnist.train.num_examples)\n",
    "    # 持续迭代\n",
    "    while step*batch_size<train_iters:\n",
    "        batch_xs,batch_ys=mnist.train.next_batch(batch_size)\n",
    "        # 对数据进行处理，使其符合输入\n",
    "        batch_xs=batch_xs.reshape((batch_size,n_steps,n_input))\n",
    "        sess.run(optimizer,{x:batch_xs,y:batch_ys})\n",
    "        # 在特定的迭代回合进行数据的输出\n",
    "        if step % display_step==0:\n",
    "            acc=sess.run(accuracy,{x:batch_xs,y:batch_ys,})\n",
    "            print('step %d,training accuracy %g'%(step,acc))\n",
    "        step+=1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}